{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning using Axolotl\n",
    "\n",
    "This notebook is an minimal example of how to finetune a LLM using [Axolotl](https://github.com/axolotl-ai-cloud/axolotl). Axolotl is a CLI tool that uses config files for different methods of LLM finetuning. We created a Python wrapper around the CLI for the end-to-end workflow for this process.\n",
    "\n",
    "In the example below, we show how you can define or load finetuning configurations to replicate the fine-tuning process from our paper, start a fine-tuning job, and push the model to Hugging Face.s\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure to run this notebook in a system with enough compute resources to run the finetuning, and follow the setup instructions in the README to install axolotl and related libraries.\n",
    "\n",
    "Let's start with loading the code components we need. The `FinetuneConfig` class holds configurations, and the `Finetune` class is used to create and run a finetuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:35:35,201] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from finetune import Finetune, FinetuneConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure to login to Hugging Face Hub to save the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160381dadcca4e1d8a23e640e7f8455e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We then load up a config to perform QLoRA finetuning of `meta-llama/Llama-2-7b-chat-hf` from in a config file stored locally. Optionally, we can assign a new field `hub_model_id`, indicating the Hugging Face model the finetuned LLM will be pushed to.\n",
    "\n",
    "For experiments in the paper we did:\n",
    "1. LoRA finetuning. The config file for LoRA finetune is `altus/finetune/examples/llama-2/lora.yml`.\n",
    "2. SFT. The config file is `altus/finetune/examples/llama-2/fft_optimized.yml`\n",
    "\n",
    "\n",
    "Next, we load a configuration file to perform LoRA fine-tuning of `meta-llama/Llama-2-7b-chat-hf`. The configuration file is stored locally. Optionally, a new field `hub_model_id` can be assigned to specify the Hugging Face model where the fine-tuned LLM will be pushed.\n",
    "\n",
    "For the experiments presented in the paper, we performed the following:\n",
    "\n",
    "1. **LoRA** Fine-tuning: The configuration file for LoRA fine-tuning is located at:\n",
    "\n",
    "`altus/finetune/examples/llama-2/lora.yml`.\n",
    "\n",
    "2. **SFT** (Supervised Fine-tuning): The configuration file for SFT is located at:\n",
    "\n",
    "`altus/finetune/examples/llama-2/fft_optimized.yml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Specify the path to your YAML file\n",
    "file_path = os.path.join(os.getcwd(), '..', 'finetune/examples/llama-2/lora.yml')\n",
    "\n",
    "# Open the file and load the data\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    config_dict = yaml.safe_load(file)  # Load the existing data\n",
    "\n",
    "config_dict['base_model'] = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "config_dict['hub_model_id'] = 'vijil/my_lora_tune'  # Add or update the model_id to push the trained model\n",
    "config_dict['eval_sample_packing'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model': 'meta-llama/Llama-2-7b-chat-hf',\n",
       " 'model_type': 'LlamaForCausalLM',\n",
       " 'tokenizer_type': 'LlamaTokenizer',\n",
       " 'load_in_8bit': True,\n",
       " 'load_in_4bit': False,\n",
       " 'strict': False,\n",
       " 'datasets': [{'path': 'mhenrichsen/alpaca_2k_test', 'type': 'alpaca'}],\n",
       " 'dataset_prepared_path': None,\n",
       " 'val_set_size': 0.05,\n",
       " 'output_dir': './lora-out',\n",
       " 'sequence_len': 4096,\n",
       " 'sample_packing': True,\n",
       " 'pad_to_sequence_len': True,\n",
       " 'adapter': 'lora',\n",
       " 'lora_model_dir': None,\n",
       " 'lora_r': 32,\n",
       " 'lora_alpha': 16,\n",
       " 'lora_dropout': 0.05,\n",
       " 'lora_target_linear': True,\n",
       " 'lora_fan_in_fan_out': None,\n",
       " 'wandb_project': None,\n",
       " 'wandb_entity': None,\n",
       " 'wandb_watch': None,\n",
       " 'wandb_name': None,\n",
       " 'wandb_log_model': None,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'micro_batch_size': 2,\n",
       " 'num_epochs': 4,\n",
       " 'optimizer': 'adamw_bnb_8bit',\n",
       " 'lr_scheduler': 'cosine',\n",
       " 'learning_rate': 0.0002,\n",
       " 'train_on_inputs': False,\n",
       " 'group_by_length': False,\n",
       " 'bf16': 'auto',\n",
       " 'fp16': None,\n",
       " 'tf32': False,\n",
       " 'gradient_checkpointing': True,\n",
       " 'early_stopping_patience': None,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'local_rank': None,\n",
       " 'logging_steps': 1,\n",
       " 'xformers_attention': None,\n",
       " 'flash_attention': True,\n",
       " 's2_attention': None,\n",
       " 'warmup_steps': 10,\n",
       " 'evals_per_epoch': 4,\n",
       " 'eval_table_size': None,\n",
       " 'eval_max_new_tokens': 128,\n",
       " 'saves_per_epoch': 1,\n",
       " 'debug': None,\n",
       " 'deepspeed': None,\n",
       " 'weight_decay': 0.0,\n",
       " 'fsdp': None,\n",
       " 'fsdp_config': None,\n",
       " 'special_tokens': None,\n",
       " 'hub_model_id': 'vijil/my_lora_tune',\n",
       " 'eval_sample_packing': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the config dict in the `FinetuneConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:35:54,661] [DEBUG] [axolotl.normalize_config:79] [PID:3716] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fed7ccc4284c69a7d5607805573a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:35:54,938] [INFO] [axolotl.normalize_config:182] [PID:3716] [RANK:0] GPU memory usage baseline: 0.000GB (+0.682GB misc)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# see all config options in './finetune/axolotl/examples/config.qmd'\n",
    "config = FinetuneConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the finetuning job\n",
    "Now simply load up the config into a `FineTune` object and kick off the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a finetune object with the config and run\n",
    "finetune = Finetune(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Job ID: 2cb97706-d81e-11ef-9844-0242ac120002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63655ea5c554a4d8d06694879950876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d73cde16c44e21ac94fc6b243aef48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb770c1e250545ae9eb12cf29cd75e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0cc52896bc428787f0b35e7d63465d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:36:00,111] [DEBUG] [axolotl.load_tokenizer:279] [PID:3716] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-21 17:36:00,112] [DEBUG] [axolotl.load_tokenizer:280] [PID:3716] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-21 17:36:00,112] [DEBUG] [axolotl.load_tokenizer:281] [PID:3716] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-21 17:36:00,112] [DEBUG] [axolotl.load_tokenizer:282] [PID:3716] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-21 17:36:00,113] [INFO] [axolotl.load_tokenizer:293] [PID:3716] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-01-21 17:36:00,113] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:3716] [RANK:0] Unable to find prepared dataset in last_run_prepared/a68bb67a61191b8469cb3317f4e3323e\u001b[39m\n",
      "[2025-01-21 17:36:00,114] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:3716] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-01-21 17:36:00,114] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:3716] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2025-01-21 17:36:00,114] [INFO] [axolotl.load_tokenized_prepared_datasets:193] [PID:3716] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5213d169b95c4b7c9f87421b7e3cdfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d9033bea774bbc824df36843a95858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c001694d20c545c3b5229afdc9bc96f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85d6355ab2e4ef5bd941179b3f22b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c03e94072834d0cb83e715af4536a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e99fe5f70d2489fb7d9d38899d5ad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Prompts (num_proc=64):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:36:09,997] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:3716] [RANK:0] merging datasets\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf0472f37264ff89b2210e9ea85bc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropping Long Sequences (num_proc=255):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449ce5c0831d434f9f4f37fcf5e8010a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Add position_id column (Sample Packing) (num_proc=255):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:36:22,744] [INFO] [axolotl.load_tokenized_prepared_datasets:423] [PID:3716] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/a68bb67a61191b8469cb3317f4e3323e\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e26c9aff7444ce894f93c01b1bf6a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:36:22,926] [DEBUG] [axolotl.log:61] [PID:3716] [RANK:0] total_num_tokens: 414_041\u001b[39m\n",
      "[2025-01-21 17:36:22,940] [DEBUG] [axolotl.log:61] [PID:3716] [RANK:0] `total_supervised_tokens: 294_246`\u001b[39m\n",
      "[2025-01-21 17:36:28,480] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 17:36:28,481] [DEBUG] [axolotl.log:61] [PID:3716] [RANK:0] data_loader_len: 6\u001b[39m\n",
      "[2025-01-21 17:36:28,481] [INFO] [axolotl.log:61] [PID:3716] [RANK:0] sample_packing_eff_est across ranks: [0.9719637357271634]\u001b[39m\n",
      "[2025-01-21 17:36:28,482] [DEBUG] [axolotl.log:61] [PID:3716] [RANK:0] sample_packing_eff_est: 0.98\u001b[39m\n",
      "[2025-01-21 17:36:28,482] [DEBUG] [axolotl.log:61] [PID:3716] [RANK:0] total_num_steps: 24\u001b[39m\n",
      "[2025-01-21 17:36:28,514] [DEBUG] [axolotl.train.log:61] [PID:3716] [RANK:0] loading tokenizer... meta-llama/Llama-2-7b-chat-hf\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:36:28,782] [DEBUG] [axolotl.load_tokenizer:279] [PID:3716] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2025-01-21 17:36:28,782] [DEBUG] [axolotl.load_tokenizer:280] [PID:3716] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2025-01-21 17:36:28,782] [DEBUG] [axolotl.load_tokenizer:281] [PID:3716] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
      "[2025-01-21 17:36:28,783] [DEBUG] [axolotl.load_tokenizer:282] [PID:3716] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2025-01-21 17:36:28,783] [INFO] [axolotl.load_tokenizer:293] [PID:3716] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-01-21 17:36:28,783] [DEBUG] [axolotl.train.log:61] [PID:3716] [RANK:0] loading model and peft_config...\u001b[39m\n",
      "[2025-01-21 17:36:29,425] [INFO] [axolotl.load_model:359] [PID:3716] [RANK:0] patching with flash attention for sample packing\u001b[39m\n",
      "[2025-01-21 17:36:29,428] [INFO] [axolotl.load_model:408] [PID:3716] [RANK:0] patching _expand_mask\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4f46ba1ae448dd879601b7cf5326c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e73ad78463743b59ee5c446e8351b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05591ccb8d74895a58cc41def9e4f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e43327ba76481bb986055ff9b85346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:39:50,392] [INFO] [accelerate.utils.modeling.get_balanced_memory:965] [PID:3716] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400f05ba92354324bfae65633a3dccf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11922aef2ae4f33a283a60aa53417f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:42:08,910] [INFO] [axolotl.load_model:720] [PID:3716] [RANK:0] GPU memory usage after model load: 6.681GB (+0.000GB cache, +1.168GB misc)\u001b[39m\n",
      "[2025-01-21 17:42:08,946] [INFO] [axolotl.load_model:771] [PID:3716] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2025-01-21 17:42:08,948] [INFO] [axolotl.load_model:780] [PID:3716] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2025-01-21 17:42:08,952] [INFO] [axolotl.load_lora:924] [PID:3716] [RANK:0] found linear modules: ['q_proj', 'k_proj', 'o_proj', 'down_proj', 'up_proj', 'v_proj', 'gate_proj']\u001b[39m\n",
      "trainable params: 79,953,920 || all params: 6,818,369,536 || trainable%: 1.172625208678628\n",
      "[2025-01-21 17:42:57,361] [INFO] [axolotl.load_model:825] [PID:3716] [RANK:0] GPU memory usage after adapters: 6.979GB (+0.851GB cache, +1.168GB misc)\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:42:57,918] [INFO] [axolotl.train.log:61] [PID:3716] [RANK:0] Pre-saving adapter config to ./lora-out\u001b[39m\n",
      "[2025-01-21 17:42:57,972] [INFO] [axolotl.train.log:61] [PID:3716] [RANK:0] Starting trainer...\u001b[39m\n",
      "[2025-01-21 17:42:58,271] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 17:42:58,273] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 17:42:58,333] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 35:10, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.314100</td>\n",
       "      <td>1.278956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.239300</td>\n",
       "      <td>1.269746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.171500</td>\n",
       "      <td>1.161470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>1.041919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>1.008087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.951219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.948300</td>\n",
       "      <td>0.918372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.859600</td>\n",
       "      <td>0.905319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.986400</td>\n",
       "      <td>0.896698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>0.891167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.877700</td>\n",
       "      <td>0.887644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.884854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.848400</td>\n",
       "      <td>0.882765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>0.881035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.880161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.880571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.918600</td>\n",
       "      <td>0.881487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:44:29,644] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:44:48,878] [INFO] [axolotl.callbacks.on_step_end:125] [PID:3716] [RANK:0] GPU memory usage while training: 7.210GB (+7.442GB cache, +1.201GB misc)\u001b[39m\n",
      "[2025-01-21 17:46:18,239] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:48:26,105] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:50:34,143] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:52:42,189] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 17:53:05,035] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 17:54:53,709] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:57:01,695] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 17:59:09,779] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:01:17,876] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 18:01:57,196] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 18:03:26,688] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:05:34,683] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:07:42,634] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:09:50,563] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 18:12:02,650] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:12:02,689] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3716] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2025-01-21 18:14:10,695] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:16:18,747] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2025-01-21 18:18:26,798] [INFO] [accelerate.accelerator.log:61] [PID:3716] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 18:18:41,447] [INFO] [axolotl.train.log:61] [PID:3716] [RANK:0] Training Completed!!! Saving pre-trained model to ./lora-out\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bash/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7d61c78359494ca9c99a466d561b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/320M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune.run() # start train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
