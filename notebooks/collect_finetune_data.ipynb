{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74169ea-64c6-4f08-a267-c4b9ce405c5b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook demonstrates the process of creating finetuning data for a consistency benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cdc73b-1fff-43fd-8041-14c2f36b15c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80824772-9da4-427a-8cd0-bfede0c99a34",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e66265-6a95-43b7-b6c6-02b013283b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from generators import CoGGenerator, BaseGenerator\n",
    "from metrics.scorer import PairwiseScorer\n",
    "from perturbations import paraphrase\n",
    "\n",
    "# Set device for Torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2313c-1993-4d69-93c8-832f97c313e3",
   "metadata": {},
   "source": [
    "## Define Parameters\n",
    "Define all key parameters that were previously set via the argparse module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdefea0-e139-49fb-8725-6f31115368ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"truthful_qa\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "aux_model_name = \"gpt-3.5-turbo\"\n",
    "variation_type = \"paraphrasing\"\n",
    "eval_agreements = \"llm,0.5;contradiction,0.5;ner,0.5\"\n",
    "metrics = [\"pp\", \"entailment\", \"bertscore\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a71f75-7417-481e-a2f5-af4c89fc9f00",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load the `truthful_qa` dataset from Hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcda9bbb-b762-42c2-8ebd-506736f90f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41bf874032940beb6a7eb96da9c9d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c9744c0884c0bada4e4a9436de1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/223k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982072f4ad9743bd9eb3978daaace5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"truthful_qa\", \"generation\")\n",
    "df = data[\"validation\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcdb7df-b241-4aa8-8f82-a2dc467dfa28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse evaluation agreements\n",
    "agreements = [\n",
    "    (x.split(\",\")[0], float(x.split(\",\")[1])) for x in eval_agreements.split(\";\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809fb78-75cc-46a3-a8df-453a9b7ee191",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "Initialize the main model which you will use to get finetuning samples from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72399a6-bec6-414a-9ace-456a19a891ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_name in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "    model = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature= 0.1,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "else:\n",
    "    task = \"text2text-generation\" if \"t5\" in model_name else \"text-generation\"\n",
    "    model = HuggingFacePipeline.from_model_id(\n",
    "        model_id=model_name,\n",
    "        task=task,\n",
    "        device=0,\n",
    "        model_kwargs={\"temperature\": 0.1, \"max_length\": 100},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2391a07-83b4-4c68-801d-fec7477cebc4",
   "metadata": {},
   "source": [
    "## Auxiliary LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d7b522-855e-4e3e-94e4-85eb5011dffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Define the Auxilliary LLM\n",
    "aux_model = None\n",
    "if \"llm\" in metrics:\n",
    "    if aux_model_name in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        aux_model = ChatOpenAI(\n",
    "            model_name=aux_model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "    else:\n",
    "        ## If not using OpenAI models, use an instruction following model like \"FlanT5\"\n",
    "        task = (\n",
    "            \"text2text-generation\"\n",
    "            if \"t5\" in aux_model_name\n",
    "            else \"text-generation\"\n",
    "        )\n",
    "        aux_model = HuggingFacePipeline.from_model_id(\n",
    "            model_id=aux_model_name,\n",
    "            task=task,\n",
    "            device=0,\n",
    "            temperature=0.1,\n",
    "            max_length=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194e7b2-bf26-4676-83ab-f7cb7c0c58a8",
   "metadata": {},
   "source": [
    "## Initialize Generators and Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88124f0a-65da-49a7-9de6-c8fd565a4a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b (last modified on Mon Jan 27 21:25:30 2025) since it couldn't be found locally at evaluate-metric--bertscore, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "a2c = CoGGenerator(model, variation_type) # Chain of Guidance Generation\n",
    "base = BaseGenerator(model, variation_type) # Ordinary Generation\n",
    "scorer = PairwiseScorer(metrics=metrics, aux_model=aux_model) # Consistency Scorer meta class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013386c-c88a-40bc-8ad4-14ac0b3b98d5",
   "metadata": {},
   "source": [
    "## Generation and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "073b422e-5e92-4144-8202-afc6313a25b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2179ea61f64babb19dd2017ef10f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/chain_of_guidance/notebooks/../perturbations/paraphrase.py:26: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  input_pp = llm(messages, stop=\"\\n\").content\n",
      "/notebooks/chain_of_guidance/notebooks/../generators/base.py:65: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=self.model, prompt=self.question_prompt)\n",
      "/notebooks/chain_of_guidance/notebooks/../generators/base.py:68: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = chain.run(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize result containers\n",
    "all_input, all_input_perturb, all_output, all_output_cons = [], [], [], []\n",
    "all_correct_output, all_scores, all_cons_scores = [], [], []\n",
    "\n",
    "# Process each input in the dataset\n",
    "for i in tqdm(range(len(df))):\n",
    "    input_text = df.question[i]\n",
    "    correct_output = df.best_answer[i]\n",
    "\n",
    "    # Generate variations (paraphrasing or sampling)\n",
    "    if variation_type == \"paraphrasing\":\n",
    "        input_perts = [\n",
    "            paraphrase.llm_prompting(input_text, method=idx) for idx in range(1, 5)\n",
    "        ]\n",
    "    else:\n",
    "        input_perts = []\n",
    "\n",
    "    # Generate outputs\n",
    "    outputs = base.generate(input_text, input_perts)\n",
    "    cons_outputs = a2c.generate(input_text, input_perts)\n",
    "\n",
    "    # Score outputs\n",
    "    score = scorer.score(input_text, outputs)\n",
    "    cons_score = scorer.score(input_text, cons_outputs)\n",
    "\n",
    "    # Store results\n",
    "    all_input.extend([input_text] * len(outputs))\n",
    "    all_input_perturb.extend(input_perts if input_perts else [\"\"] * len(outputs))\n",
    "    all_output.extend(outputs)\n",
    "    all_output_cons.extend(cons_outputs)\n",
    "    all_correct_output.extend([correct_output] * len(outputs))\n",
    "    all_scores.extend([score] * len(outputs))\n",
    "    all_cons_scores.extend([cons_score] * len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098697ae-52d9-4e73-b1b6-bc5983ef3bcf",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24d2a8-68f0-43bf-a6b8-15d1de773a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "res_df = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": all_input,\n",
    "        \"input_pert\": all_input_perturb,\n",
    "        \"outputs_correct\": all_correct_output,\n",
    "        \"output_sampled\": all_output,\n",
    "        \"output_consistent\": all_output_cons,\n",
    "        \"score\": all_scores,\n",
    "        \"score_consistent\": all_cons_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_file = f\"result_{model_name.replace('/', '')}_{variation_type}.csv\"\n",
    "res_df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
